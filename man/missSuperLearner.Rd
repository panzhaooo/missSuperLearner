% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/missSuperLearner.R
\name{missSuperLearner}
\alias{missSuperLearner}
\title{Super Learner Prediction Function That Handles Missing Data}
\usage{
missSuperLearner(
  Y,
  X,
  newX = NULL,
  family = stats::gaussian(),
  SL.library,
  imputeAlgo = c("mean", "median", "mice"),
  mice.params = list(m = 5),
  max.try = 10,
  method = "method.NNLS",
  id = NULL,
  verbose = FALSE,
  control = list(),
  cvControl = list(),
  obsWeights = NULL,
  outputX = FALSE,
  env = parent.frame()
)
}
\arguments{
\item{Y}{The outcome in the training data set. Must be a numeric vector.}

\item{X}{The predictor variables in the training data set, usually a
data.frame.}

\item{newX}{The predictor variables in the validation data set. The
structure should match X. If missing, uses X for newX.}

\item{family}{Currently allows \code{gaussian} or \code{binomial} to
describe the error distribution. Link function information will be ignored
and should be contained in the method argument below.}

\item{SL.library}{Either a character vector of prediction algorithms or a
list containing character vectors. See details below for examples on the
structure. A list of functions included in the SuperLearner package can be
found with \code{listWrappers()}.}

\item{imputeAlgo}{The imputation algorithms. A character vector of imputation 
algorithms.}

\item{mice.params}{A list of arguments specified for the \code{mice} function.}

\item{max.try}{The maximal number of tries to generate valid cross-validation
folds with missing data.}

\item{method}{A list (or a function to create a list) containing details on
estimating the coefficients for the super learner and the model to combine
the individual algorithms in the library. See \code{?method.template} for
details.  Currently, the built in options are either "method.NNLS" (the
default), "method.NNLS2", "method.NNloglik", "method.CC_LS",
"method.CC_nloglik", or "method.AUC".  NNLS and NNLS2 are non-negative least
squares based on the Lawson-Hanson algorithm and the dual method of Goldfarb
and Idnani, respectively.  NNLS and NNLS2 will work for both gaussian and
binomial outcomes.  NNloglik is a non-negative binomial likelihood
maximization using the BFGS quasi-Newton optimization method. NN* methods
are normalized so weights sum to one. CC_LS uses Goldfarb and Idnani's
quadratic programming algorithm to calculate the best convex combination of
weights to minimize the squared error loss. CC_nloglik calculates the convex
combination of weights that minimize the negative binomial log likelihood on
the logistic scale using the sequential quadratic programming algorithm.
AUC, which only works for binary outcomes, uses the Nelder-Mead method via
the optim function to minimize rank loss (equivalent to maximizing AUC).}

\item{id}{Optional cluster identification variable. For the cross-validation
splits, \code{id} forces observations in the same cluster to be in the same
validation fold. \code{id} is passed to the prediction and screening
algorithms in SL.library, but be sure to check the individual wrappers as
many of them ignore the information.}

\item{verbose}{logical; TRUE for printing progress during the computation
(helpful for debugging).}

\item{control}{A list of parameters to control the estimation process.
Parameters include \code{saveFitLibrary} and \code{trimLogit}. See
\code{\link{SuperLearner.control}} for details.}

\item{cvControl}{A list of parameters to control the cross-validation
process. Parameters include \code{V}, \code{stratifyCV}, \code{shuffle} and
\code{validRows}. See \code{\link{SuperLearner.CV.control}} for details.}

\item{obsWeights}{Optional observation weights variable. As with \code{id}
above, \code{obsWeights} is passed to the prediction and screening
algorithms, but many of the built in wrappers ignore (or can't use) the
information. If you are using observation weights, make sure the library you
specify uses the information.}

\item{outputX}{logical; TRUE for outputting \code{X} in the returned list 
(helpful for future prediction).}

\item{env}{Environment containing the learner functions. Defaults to the
calling environment.}
}
\value{
\item{call}{ The matched call. } 
\item{libraryNames}{ A character
vector with the names of the algorithms in the library. The format is
'predictionAlgorithm_screeningAlgorithm' with '_All' used to denote the
prediction algorithm run on all variables in X. } 
\item{SL.library}{ Returns \code{SL.library} in the same format as the 
argument with the same name above. } 
\item{imputeAlgo}{ Returns \code{imputeAlgo} in the same format as the 
argument with the same name above. }
\item{mice.params}{ Returns \code{mice.params} in the same format as the 
argument with the same name above. }
\item{X}{ Returns \code{X} in the same format as the 
argument with the same name above. }
\item{SL.predict}{ The predicted values from the super learner for
the rows in \code{newX}. } 
\item{coef}{ Coefficients for the super learner.
} 
\item{library.predict}{ A matrix with the predicted values from each
algorithm in \code{SL.library} for the rows in \code{newX}. } 
\item{Z}{ The
Z matrix (the cross-validated predicted values for each algorithm in
\code{SL.library}). } 
\item{cvRisk}{ A numeric vector with the V-fold
cross-validated risk estimate for each algorithm in \code{SL.library}. Note
that this does not contain the CV risk estimate for the SuperLearner, only
the individual algorithms in the library. } 
\item{family}{ Returns the
\code{family} value from above } 
\item{fitLibrary}{ A list with the fitted
objects for each algorithm in \code{SL.library} on the full training data
set. } 
\item{cvFitLibrary}{ A list with fitted objects for each algorithm in
\code{SL.library} on each of \code{V} different training data sets.  }
\item{varNames}{ A character vector with the names of the variables in
\code{X}. } 
\item{validRows}{ A list containing the row numbers for the
V-fold cross-validation step. } 
\item{method}{ A list with the method
functions. } 
\item{whichScreen}{ A logical matrix indicating which variables
passed each screening algorithm. } 
\item{control}{ The \code{control} list.
} 
\item{cvControl}{ The \code{cvControl} list. } 
\item{errorsInCVLibrary}{ A
logical vector indicating if any algorithms experienced an error within the
CV step. } 
\item{errorsInLibrary}{ A logical vector indicating if any
algorithms experienced an error on the full data. } 
\item{env}{ Environment
passed into function which will be searched to find the learner functions.
Defaults to the calling environment.  } 
\item{times}{ A list that contains
the execution time of the SuperLearner, plus separate times for model
fitting and prediction.  }
}
\description{
A Prediction Function for the Super Learner that handles missing data. The
\code{missSuperLearner} function takes a training set pair (X, Y) where X may
contain missing data (NAs), and returns the predicted values based on a 
validation set.
}
\details{
\code{missSuperLearner} fits the super learner prediction algorithm that handles
missing data. The weights for each prediction algorithm in \code{SL.library}, 
in combination with each imputation algorithm in \code{imputeAlgo}, is estimated, 
along with the fit of each algorithm.

The imputation algorithms. To incorporate the imputation step into the 
cross-validation scheme, three widely used methods are considered: \code{mean},
\code{median}, and multivariate imputation by chained equations (\code{mice}).

The pre-screen algorithms. These algorithms first rank the variables in
\code{X} based on either a univariate regression p-value of the
\code{randomForest} variable importance.  A subset of the variables in
\code{X} is selected based on a pre-defined cut-off.  With this subset of
the X variables, the algorithms in \code{SL.library} are then fit.

The \code{missSuperLearner} package inherits a few prediction and screening 
algorithm wrappers from the \code{SuperLearner} package. The full list of 
wrappers can be viewed with \code{listWrappers()}. The design of the 
\code{missSuperLearner} package is such that the user can easily add their 
own wrappers. 

Note that we add a new wrapper function \code{SL.grf} for the generalized
random forests (GRF) algorithm from the \code{grf} package. GRF can handle 
missing data implicitly using the missing incorporated in attributes criterion
(MIA; Twala et al., 2008).
}
\section{Functions}{
\itemize{
\item \code{missSuperLearner()}: A Super Learner Prediction Function That Handles Missing Data

}}
\examples{

\dontrun{
## simulate data
set.seed(1)
## training set
n <- 500
p <- 10
X <- matrix(rnorm(n * p), nrow = n, ncol = p)
colnames(X) <- paste("X", 1:p, sep="")
X <- data.frame(X)
Y <- X[, 1] + sqrt(abs(X[, 2] * X[, 3])) + X[, 2] - X[, 3] + rnorm(n)

## test set
m <- 1000
newX <- matrix(rnorm(m * p), nrow = m, ncol = p)
colnames(newX) <- paste("X", 1:p, sep="")
newX <- data.frame(newX)
newY <- newX[, 1] + sqrt(abs(newX[, 2] * newX[, 3])) + newX[, 2] -
  newX[, 3] + rnorm(m)
  
## missing values in X and newX
for (i in 1:p) {
  X[runif(n) < 0.1, i] <- NA
  newX[runif(n) < 0.1, i] <- NA
}

# generate Library and run Super Learner
SL.library <- c("SL.glm", "SL.ranger", "SL.gam",
                "SL.grf", "SL.mean")
test <- missSuperLearner(Y = Y, X = X, newX = newX, SL.library = SL.library,
                         imputeAlgo = c("mean", "median", "mice"),
                         verbose = TRUE, method = "method.NNLS")
test

# library with screening
SL.library <- list(c("SL.glmnet", "All"), 
                   c("SL.glm", "screen.randomForest", "All", "screen.SIS"), 
                   "SL.randomForest", 
                   c("SL.polymars", "All"), 
                   "SL.mean")
test <- missSuperLearner(Y = Y, X = X, newX = newX, SL.library = SL.library,
                         imputeAlgo = c("mean", "median", "mice"),
                         verbose = TRUE, method = "method.NNLS")
test

# binary outcome
set.seed(1)
N <- 200
X <- matrix(rnorm(N*10), N, 10)
X <- as.data.frame(X)
Y <- rbinom(N, 1, plogis(.2*X[, 1] + .1*X[, 2] - .2*X[, 3] +
  .1*X[, 3]*X[, 4] - .2*abs(X[, 4])))
  
for (i in 1:10) {
  X[runif(n) < 0.1, i] <- NA
}

SL.library <- c("SL.glmnet", "SL.glm", "SL.knn", "SL.mean")

# least squares loss function
test.NNLS <- missSuperLearner(Y = Y, X = X, SL.library = SL.library,
                              verbose = TRUE, method = "method.NNLS", family = binomial())
test.NNLS

# negative log binomial likelihood loss function
test.NNloglik <- missSuperLearner(Y = Y, X = X, SL.library = SL.library,
                                  verbose = TRUE, method = "method.NNloglik", family = binomial())
test.NNloglik

# 1 - AUC loss function
test.AUC <- missSuperLearner(Y = Y, X = X, SL.library = SL.library,
                             verbose = TRUE, method = "method.AUC", family = binomial())
test.AUC

}

}
\author{
Pan Zhao \email{pan.zhao@inria.fr}
}
\keyword{models}
