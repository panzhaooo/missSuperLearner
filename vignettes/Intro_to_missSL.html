<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<meta name="author" content="Pan Zhao, Nicolas Gatulle, Julie Josse and Antoine Chambaz" />

<meta name="date" content="2022-09-05" />

<title>Introduction to missSuperLearner</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>







<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">Introduction to missSuperLearner</h1>
<h4 class="author">Pan Zhao, Nicolas Gatulle, Julie Josse and Antoine
Chambaz</h4>
<h4 class="date">September 5, 2022</h4>


<div id="TOC">
<ul>
<li><a href="#introduction" id="toc-introduction"><span class="toc-section-number">1</span> Introduction</a></li>
<li><a href="#basic-setup" id="toc-basic-setup"><span class="toc-section-number">2</span> Basic Setup</a>
<ul>
<li><a href="#notation" id="toc-notation"><span class="toc-section-number">2.1</span> Notation</a></li>
<li><a href="#super-learning" id="toc-super-learning"><span class="toc-section-number">2.2</span> Super Learning</a></li>
</ul></li>
<li><a href="#super-learning-with-missing-data" id="toc-super-learning-with-missing-data"><span class="toc-section-number">3</span> Super Learning with Missing Data</a>
<ul>
<li><a href="#imputation" id="toc-imputation"><span class="toc-section-number">3.1</span> Imputation</a></li>
<li><a href="#super-learning-algorithm-that-handles-missing-data" id="toc-super-learning-algorithm-that-handles-missing-data"><span class="toc-section-number">3.2</span> Super Learning Algorithm that
Handles Missing Data</a></li>
</ul></li>
<li><a href="#references" id="toc-references"><span class="toc-section-number">4</span> References</a></li>
</ul>
</div>

<div id="introduction" class="section level1" number="1">
<h1><span class="header-section-number">1</span> Introduction</h1>
<p>This vignette introduces the basic concepts for the super learning
algorithm that is able to handle missing data, which is implemented in
the <code>missSuperLearner</code> package.</p>
</div>
<div id="basic-setup" class="section level1" number="2">
<h1><span class="header-section-number">2</span> Basic Setup</h1>
<p>In this section, we introduce the notation and review the super
learning algorithm [1, 2].</p>
<div id="notation" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> Notation</h2>
<p>Observe the independent and identically distributed data points <span class="math inline">\(O_i = (Y_i, X_i), i = 1, ... ,n\)</span>, where
<span class="math inline">\(Y\)</span> is the outcome of interest and
<span class="math inline">\(X\)</span> is a <span class="math inline">\(p\)</span>-dimensional covariate. The objective is
to estimate the function <span class="math inline">\(f_0(X) = E(Y \,|\,
X)\)</span>. The function can be expressed as the minimizer of the
expected loss: <span class="math display">\[f_0(X) = \arg\min_f E[L(O,
f(X))]\]</span> where the loss function is, for example, the squared
error loss <span class="math inline">\(L_2 = (Y - f(X))^2\)</span>.</p>
<p>Let <span class="math inline">\(P_n\)</span> denote the empirical
distribution over the data.</p>
</div>
<div id="super-learning" class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> Super Learning</h2>
<p>For a given problem, a library of prediction algorithms can be
proposed. A library is simply a collection of algorithms. The algorithms
in the library should come from contextual knowledge and a large set of
default algorithms. We use algorithm in the general sense as any mapping
from the data into a predictor. The algorithms may range from a simple
linear regression model to a multi-step algorithm involving screening
covariates, optimizing tuning parameters, and selecting a working model
among a large class of candidate working models. As long as the
algorithm takes the observed data and outputs a predicted value we
consider it a prediction algorithm. For example, the library may include
least squares regression estimators for a large class of regression
working models indexed by subsets of the covariates, algorithms indexed
by set values of the fine-tuning parameters for a collection of values,
algorithms using internal cross-validation to set fine-tuning
parameters, algorithms coupled with screening procedures to reduce the
dimension of the covariate vector, and so on.</p>
<p>Denote the library <span class="math inline">\(\mathcal{L}\)</span>
and the cardinality of <span class="math inline">\(\mathcal{L}\)</span>
as <span class="math inline">\(K\)</span>. Consider the super learning
algorithm:</p>
<ol style="list-style-type: decimal">
<li><p>Fit each algorithm in the library <span class="math inline">\(\mathcal{L}\)</span> on the entire data set <span class="math inline">\(\mathcal{O} = \{O_i : i = 1, \ldots, n\}\)</span>
to estimate <span class="math inline">\(\hat{f}_k (X), k = 1, \ldots,
K\)</span>.</p></li>
<li><p>Split the data set <span class="math inline">\(\mathcal{O}\)</span> into a training and
validation sample, according to a <span class="math inline">\(V\)</span>-fold cross-validation scheme: split the
ordered <span class="math inline">\(n\)</span> observations into <span class="math inline">\(V\)</span> equal-size groups, let the <span class="math inline">\(v\)</span>-th group be the validation sample, and
the remaining group the training sample, <span class="math inline">\(v =
1, \ldots, V\)</span>. Define <span class="math inline">\(T(v)\)</span>
to be the <span class="math inline">\(v\)</span>-th training data split
and <span class="math inline">\(V(v)\)</span> to be the corresponding
validation data split. <span class="math inline">\(T(v) = \mathcal{O}
\backslash V(v), v = 1, \ldots, V\)</span></p></li>
<li><p>For the <span class="math inline">\(v\)</span>-th fold, fit each
algorithm in <span class="math inline">\(\mathcal{L}\)</span> on <span class="math inline">\(T(v)\)</span> and save the predictions on the
corresponding validation data, <span class="math inline">\(\hat{f}_{k,T(v)} (X_i), O_i \in V(v), v = 1,
\ldots, V\)</span>.</p></li>
<li><p>Stack the predictions from each algorithm together to create a
<span class="math inline">\(n \times K\)</span> matrix <span class="math inline">\(Z =\left\{\hat{f}_{k,T(v)} (X_{V(v)}), v = 1,
\ldots, V, k = 1, \ldots, K\right\}\)</span>, where we used the notation
<span class="math inline">\(X_{V(v)} = (X_i: O_i \in V(v))\)</span> for
the covariate vectors of the <span class="math inline">\(V(v)\)</span>-validation sample.</p></li>
<li><p>Propose a family of weighted combinations of the candidate
estimators indexed by weight vector <span class="math inline">\(\alpha\)</span>: <span class="math display">\[m(z|a) = \sum_{k=1}^K \alpha_k \,
\hat{f}_{k,T(v)} (X_{V(v)}), \alpha_k \geq 0 \, \forall k, \sum_{k=1}^K
\alpha_k = 1.\]</span></p></li>
<li><p>Determine the <span class="math inline">\(\alpha\)</span> that
minimizes the cross-validated risk of the candidate estimator <span class="math inline">\(\sum_{k=1}^K \alpha_k \, \hat{f_k}\)</span> over
all allowed <span class="math inline">\(\alpha\)</span>-combinations:
<span class="math display">\[\hat{\alpha} = \arg\min_{\alpha}
\sum_{i=1}^n (Y_i - m(z_i | \alpha))^2.\]</span></p></li>
<li><p>Combine <span class="math inline">\(\hat{\alpha}\)</span> with
<span class="math inline">\(\hat{f_k} (X), k = 1, \ldots, K\)</span>
according to the family <span class="math inline">\(m(z|a)\)</span> of
weighted combinations to create the final super learner fit: <span class="math display">\[\hat{f}_{SL} (X) = \sum_{k=1}^K \hat{\alpha}_k \,
\hat{f_k} (X).\]</span></p></li>
</ol>
<p>In summary, Super learner performs asymptotically as well as best
possible weighted combination.</p>
</div>
</div>
<div id="super-learning-with-missing-data" class="section level1" number="3">
<h1><span class="header-section-number">3</span> Super Learning with
Missing Data</h1>
<p>Missing data are ubiquitous in many real data analysis tasks. There
exists extensive literature on <strong>imputation</strong> and inference
with missing data [3]. The <em>Missing At Random</em> (MAR) assumption
is mostly used in theory and practices, which allows us to maximize the
likelihood of observed data without considering the missing mechanism.
When the data is <em>Missing Not At Random</em>, where missingness
depends on the unobserved values, the identification problem complicates
the analysis.</p>
<p>Learning predictive models with missing values poses distinct
challenges compared to inference tasks [4]. The most common practice
however remains by far to use off-the-shelf methods first for imputation
of missing values and second for supervised-learning on the resulting
completed data.</p>
<p>A systematic analysis of Impute-the-Regress procedures in a general
setting: non-linear response function and any missingness mechanism (no
MAR assumptions) in [5] shows that Impute-then-Regress procedures are
Bayes optimal for all missing data mechanisms and for almost all
imputation functions, whatever the number of variables that may be
missing.</p>
<div id="imputation" class="section level2" number="3.1">
<h2><span class="header-section-number">3.1</span> Imputation</h2>
<p>There are many popular imputation methods, such as
<code>softImpute</code> (matrix completion via iterative
soft-thresholded SVD) and <code>missForest</code> (nonparametric missing
value imputation using random forest). See the <a href="https://rmisstastic.netlify.app/">R-miss-tastic</a> website for
comprehensive references. However, in order to incorporate the
imputation step into the cross-validation scheme, three methods are
implemented in our package.</p>
<ul>
<li><p><strong>Mean Imputation</strong> For <span class="math inline">\(v = 1, \ldots, V\)</span>, we use the mean of the
training sample <span class="math inline">\(T(v)\)</span> to impute the
missing values. Note that for categorical/integer variables, the mode is
used instead.</p></li>
<li><p><strong>Median Imputation</strong> For <span class="math inline">\(v = 1, \ldots, V\)</span>, we use the median of
the training sample <span class="math inline">\(T(v)\)</span> to impute
the missing values. Note that for categorical/integer variables, the
mode is used instead.</p></li>
<li><p><strong>Multivariate Imputation by Chained Equations
(MICE)</strong> The <code>mice</code> package implements a method to
deal with missing data, and creates multiple imputations for
multivariate missing data. The method is based on <em>Fully Conditional
Specification</em>, where each incomplete variable is imputed by a
separate model. Note that here we only use the training sample to fit
the models.</p></li>
</ul>
</div>
<div id="super-learning-algorithm-that-handles-missing-data" class="section level2" number="3.2">
<h2><span class="header-section-number">3.2</span> Super Learning
Algorithm that Handles Missing Data</h2>
<p>We consider the same super learning algorithm as introduced in
Section 2.2, except that the functions class is <span class="math display">\[\mathcal{F}^\ast = \left\{f^\ast : f^\ast
(X^\ast) = f^\ast_{c} \circ f^\ast_{b} \circ f^\ast_{a} (X^\ast)
\right\},\]</span> where <span class="math inline">\(X^\ast\)</span>
denotes the covariates with missing values, and <span class="math inline">\(f^\ast_{a}, f^\ast_{b}, f^\ast_{c}\)</span> are
imputation, screening and prediction functions, respectively.</p>
</div>
</div>
<div id="references" class="section level1" number="4">
<h1><span class="header-section-number">4</span> References</h1>
<p>[1] Van der Laan, M. J., Polley, E. C., &amp; Hubbard, A. E. (2007).
<strong>Super learner.</strong> <em>Statistical applications in genetics
and molecular biology</em>, 6(1). <a href="https://doi.org/10.2202/1544-6115.1309" class="uri">https://doi.org/10.2202/1544-6115.1309</a></p>
<p>[2] Polley, E. C., &amp; Van Der Laan, M. J. (2010). <strong>Super
learner in prediction.</strong> <em>U.C. Berkeley Division of
Biostatistics Working Paper Series</em>. Paper 226. <a href="https://biostats.bepress.com/ucbbiostat/paper266/" class="uri">https://biostats.bepress.com/ucbbiostat/paper266/</a></p>
<p>[3] Little, R. J., &amp; Rubin, D. B. (2019). <strong>Statistical
analysis with missing data</strong> (Vol. 793). John Wiley &amp;
Sons.</p>
<p>[4] Josse, J., Prost, N., Scornet, E., &amp; Varoquaux, G. (2019).
<strong>On the consistency of supervised learning with missing
values.</strong> <em>arXiv preprint</em> <a href="https://arxiv.org/abs/1902.06931">arXiv:1902.06931.</a></p>
<p>[5] Le Morvan, M., Josse, J., Scornet, E., &amp; Varoquaux, G.
(2021). <strong>Whatâ€™s a good imputation to predict with missing
values?.</strong> <em>Advances in Neural Information Processing
Systems</em>, 34, 11530-11540. <a href="https://arxiv.org/abs/2106.00311" class="uri">https://arxiv.org/abs/2106.00311</a></p>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
