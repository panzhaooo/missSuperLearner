---
title: "Guide to missSuperLearner"
author: Pan Zhao, Nicolas Gatulle, Julie Josse and Antoine Chambaz
date: September 5, 2022
output:
  rmarkdown::html_vignette:
    toc: true
    toc_depth: 2
    number_sections: true
vignette: >
  %\VignetteIndexEntry{Guide to missSuperLearner}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(missSuperLearner)

# to save compilation time, some code chunks will not be executed
do_it_compile <- FALSE
```

# Introduction

This vignette is a practical guide for using the `missSuperLearner` package, which 
builds on the `SuperLearner` package. If you are unfamiliar with the `SuperLearner` 
package, it is advisable to read first this [Guide to SuperLearner](https://cran.r-project.org/web/packages/SuperLearner/vignettes/Guide-to-SuperLearner.html) 
that this guide follows closely.

# Dataset

We use the `diabetes` dataset which is available from the `VIM` package.

```{r}
# Load the dataset from the VIM package
data(diabetes, package = "VIM")

# Info of the diabetes dataset
?VIM::diabetes

# Summary of the dataset
summary(diabetes)

# Check for missing data
colSums(is.na(diabetes))
```

```{r}
# Get the outcome variable from the data
outcome <- diabetes$Outcome

# Create a dataframe to contain the explanatory variables
data <- subset(diabetes, select = -Outcome)

# Check structure of our dataframe
str(data)

# Review the dimensions
dim(data)
```

```{r}
# Set a seed for reproducibility
set.seed(1)

# Reduce to a dataset of 400 observations to speed up model fitting
train_obs <- sample(nrow(data), 400)
# training sample
x_train <- data[train_obs, ]

# Create a holdout set for evaluating model performance.
# Note: cross-validation is even better than a single holdout sample.
x_holdout <- data[-train_obs, ]

# The outcome must be a numeric vector
outcome <- as.numeric(outcome == "yes")

y_train <- outcome[train_obs]
y_holdout <- outcome[-train_obs]

# Review the outcome variable distribution.
table(y_train, useNA = "ifany")
```

# Review available models

```{r}
SuperLearner::listWrappers()
```

Note that we add a new `grf` learner ([generalized random forests](https://grf-labs.github.io/grf/index.html)). Look at the code for a model.
```{r}
SL.grf
```

For maximum accuracy one might try at least the following models: glmnet, ranger, XGBoost, SVM, and bartMachine. These should ideally be tested with multiple hyperparameter settings for each algorithm.

# Fit individual models

Let's fit 2 separate models: lasso (sparse, penalized OLS) and random forest. We specify `family = binomial()` because we are predicting a binary outcome, aka classification. With a continuous outcome we would specify `family = gaussian()`.

```{r, eval=do_it_compile}
set.seed(1)

# Fit lasso model
sl_lasso <- missSuperLearner(Y = y_train, X = x_train, family = "binomial",
                             method = SuperLearner::method.NNLS,
                             imputeAlgo = c("mean", "median", "mice"),
                             SL.library = "SL.glmnet")
sl_lasso
```

```{r, eval=do_it_compile}
# Review the elements in the missSuperLearner object
names(sl_lasso)
```

```{r, eval=do_it_compile}
# Here is the risk of the best model (discrete SuperLearner winner).
sl_lasso$cvRisk[which.min(sl_lasso$cvRisk)]
```

```{r, eval=do_it_compile}
# Here is the raw glmnet result object:
str(sl_lasso$fitLibrary$SL.glmnet_All_mean$object, max.level = 1)
```

```{r, eval=do_it_compile}
# Fit random forest.
sl_rf <- missSuperLearner(Y = y_train, X = x_train, family = binomial(),
                          imputeAlgo = c("mean", "median", "mice"),
                          SL.library = "SL.ranger")
sl_rf
```

Risk is a measure of model accuracy or performance. We want our models to minimize the estimated risk, which means the model is making the fewest mistakes in its prediction. It's basically the mean-squared error in a regression model, but you can customize it if you want.

missSuperLearner is using cross-validation to estimate the risk on future data. By default it uses 10 folds; use the cvControl argument to customize.

The coefficient column tells us the weight or importance of each individual learner in the overall ensemble. By default the weights are always greater than or equal to 0 and sum to 1. In this case we only have one algorithm so the coefficient has to be 1. If a coefficient is 0 it means that the algorithm isn't being used in the missSuperLearner ensemble.

# Fit multiple models

Instead of fitting the models separately and looking at the performance (lowest risk), we can fit them simultaneously. missSuperLearner will then tell us which one is best (discrete winner) and also create a weighted average of multiple models.

We include the mean of `Y` ("SL.mean") as a benchmark algorithm. It is a very simple prediction so the more complex algorithms should do better than the sample mean. We hope to see that it isn't the best single algorithm (discrete winner) and has a low weight in the weighted-average ensemble. If it is the best algorithm something has likely gone wrong.

```{r, eval=do_it_compile}
set.seed(1)
sl <- missSuperLearner(Y = y_train, X = x_train, family = binomial(),
                       imputeAlgo = c("mean", "median", "mice"),
                       SL.library = c("SL.mean", "SL.glmnet", "SL.ranger", "SL.grf"))
sl
```

```{r, eval=do_it_compile}
# Review how long it took to run the SuperLearner:
sl$times$everything
```

Again, the coefficient is how much weight missSuperLearner puts on that model in the weighted-average. So if coefficient = 0 it means that model is not used at all. Here we see that random forest with median imputation is given the most weight, following by lasso with median imputation.

So we have an automatic ensemble of multiple learners based on the cross-validated performance of those learners, nice!

# Predict on new data

Now that we have an ensemble let's predict back on our holdout dataset and review the results.

```{r, eval=do_it_compile}
# Predict back on the holdout dataset.
# onlySL is set to TRUE so we don't fit algorithms that had weight = 0, saving computation.
pred <- predict(sl, newdata = x_holdout, X = x_train, onlySL = TRUE)

# Check the structure of this prediction object.
str(pred)
```

```{r, eval=do_it_compile}
# Review the columns of $library.predict.
summary(pred$library.predict)
```

```{r, eval=do_it_compile}
# Histogram of our predicted values.
ggplot2::qplot(pred$pred[, 1]) + ggplot2::theme_minimal()
```

```{r, eval=do_it_compile}
# Scatterplot of original values (0, 1) and predicted values.
# Ideally we would use jitter or slight transparency to deal with overlap.
ggplot2::qplot(y_holdout, pred$pred[, 1]) + ggplot2::theme_minimal()
```

```{r, eval=do_it_compile}
# Review AUC - Area Under Curve
pred_rocr <- ROCR::prediction(pred$pred, y_holdout)
auc <- ROCR::performance(pred_rocr, measure = "auc", x.measure = "cutoff")@y.values[[1]]
auc
```

AUC can range from 0.5 (no better than chance) to 1.0 (perfect). So at 0.86 we are looking pretty good!

# Fit ensemble with external cross-validation

What we don't have yet is an estimate of the performance of the ensemble itself. Right now we are just hopeful that the ensemble weights are successful in improving over the best single algorithm.

In order to estimate the performance of the missSuperLearner ensemble we need an "external" layer of cross-validation, also called nested cross-validation. We generate a separate holdout sample that we don't use to fit the missSuperLearner, which allows it to be a good estimate of the missSuperLearner's performance on unseen data. Typically we would run 10 or 20-fold external cross-validation, but even 5-fold is reasonable.

Another nice result is that we get standard errors on the performance of the individual algorithms and can compare them to the missSuperLearner.

```{r, eval=do_it_compile}
set.seed(1)

# Don't have timing info for the CV.SuperLearner unfortunately.
# So we need to time it manually.

system.time({
  # This will take about twice as long as the previous SuperLearner.
  cv_sl <- CV.missSuperLearner(Y = y_train, X = x_train, family = binomial(),
                               # For a real analysis we would rather use V = 10.
                               cvControl = list(V = 2), innerCvControl = list(list(V=2)),
                               imputeAlgo = c("mean", "median", "mice"),
                               SL.library = c("SL.mean", "SL.glmnet", "SL.ranger"))
})
```

```{r, eval=do_it_compile}
# We run summary on the cv_sl object rather than simply printing the object.
summary(cv_sl)
```

```{r, eval=do_it_compile}
# Review the distribution of the best single learner as external CV folds.
table(simplify2array(cv_sl$whichDiscreteSL))
```

```{r, eval=do_it_compile}
# Plot the performance with 95% CIs (use a better ggplot theme).
plot(cv_sl) + ggplot2::theme_bw()
```

We see two missSuperLearner results: "Super Learner" and "Discrete SL". "Discrete SL" chooses the best single learner - in this case `SL.glmnet` (lasso) with median imputation. "Super Learner" takes a weighted average of the learners using the coefficients/weights that we examined earlier. In general "Super Learner" should perform a little better than "Discrete SL".

We see based on the outer cross-validation that missSuperLearner is statistically tying with the best algorithm. Our benchmark learner "SL.mean" shows that we get a nice improvement over a naive guess based only on the mean. We could also add "SL.glm" to compare to logistic regression.

# Customize a model hyperparameter

Hyperparameters are the configuration settings for an algorithm. OLS has no hyperparameters but essentially every other algorithm does.

There are two ways to customize a hyperparameter: make a new learner function, or use create.Learner().

Let's make a variant of random forest that fits more trees, which may increase our accuracy and can't hurt it (outside of small random variation).

```{r, eval=do_it_compile}
# Review the function argument defaults at the top.
SuperLearner::SL.ranger
```

```{r, eval=do_it_compile}
# Create a new function that changes just the num.trees argument.
# (We could do this in a single line.)
# "..." means "all other arguments that were sent to the function"
SL.rf.better = function(...) {
  SuperLearner::SL.ranger(..., num.trees = 1000)
}

set.seed(1)

# Fit the CV.missSuperLearner.
# We use V = 3 to save computation time; for a real analysis we would rather use V = 10 or 20.
cv_sl <- CV.missSuperLearner(Y = y_train, X = x_train, family = binomial(), cvControl = list(V = 3),
                             imputeAlgo = c("mean", "median", "mice"),
                             SL.library = c("SL.mean", "SL.glmnet", "SL.rf.better", "SL.ranger"))
```

```{r, eval=do_it_compile}
# Review results.
summary(cv_sl)
```

Looks like our new RF is improving performance a bit. Sometimes the performance may not be improved if the original hyperparameter had already reached the performance plateau - a maximum accuracy that RF can achieve unless other settings are changed (e.g. max nodes).

For comparison we can do the same hyperparameter customization with create.Learner().

```{r, eval=do_it_compile}
# Customize the defaults for random forest.
SL.ranger <- SuperLearner::SL.ranger
predict.SL.ranger <- SuperLearner::predict.SL.ranger
learners <- SuperLearner::create.Learner("SL.ranger", params = list(num.trees = 1000))

# Look at the object.
learners
```

```{r, eval=do_it_compile}
# List the functions that were created
learners$names
```

```{r, eval=do_it_compile}
# Review the code that was automatically generated for the function.
# Notice that it's exactly the same as the function we made manually.
SL.ranger_1
```

```{r, eval=do_it_compile}
set.seed(1)

# Fit the CV.missSuperLearner.
# We use V = 3 to save computation time; for a real analysis we  would rather use V = 10 or 20.
cv_sl <- CV.missSuperLearner(Y = y_train, X = x_train, family = binomial(), V = 3,
                             imputeAlgo = c("mean", "median", "mice"),
                             SL.library = c("SL.mean", "SL.glmnet", learners$names, "SL.ranger"))

# Review results.
summary(cv_sl)
```

We get exactly the same results between the two methods of creating a custom learner.

# Test algorithm with multiple hyperparameter settings

The performance of an algorithm varies based on its hyperparamters, which again are its configuration settings. Some algorithms may not vary much, and others might have far better or worse performance for certain settings. Often we focus our attention on 1 or 2 hyperparameters for a given algorithm because they are the most important ones.

For random forest there are two particularly important hyperparameters: mtry and maximum leaf nodes. Mtry is how many features are randomly chosen within each decision tree node - in other words, each time the tree considers making a split. Maximum leaf nodes controls how complex each tree can get.

Let's try 3 different mtry options.

```{r, eval=do_it_compile}
# sqrt(p) is the default value of mtry for classification.
floor(sqrt(ncol(x_train)))
```

```{r, eval=do_it_compile}
# Let's try 3 multiplies of this default: 0.5, 1, and 2.
(mtry_seq <- floor(sqrt(ncol(x_train)) * c(0.5, 1, 2)))
```

```{r, eval=do_it_compile}
learners <- SuperLearner::create.Learner("SL.ranger", tune = list(mtry = mtry_seq))

# Review the resulting object
learners
```

```{r, eval=do_it_compile}
# Check code for the learners that were created.
SL.ranger_1
```

```{r, eval=do_it_compile}
SL.ranger_2
```

```{r, eval=do_it_compile}
SL.ranger_3
```

```{r, eval=do_it_compile}
set.seed(1)

# Fit the CV.SuperLearner.
# We use V = 3 to save computation time; for a real analysis we would rather use V = 10 or 20.
cv_sl <- CV.missSuperLearner(Y = y_train, X = x_train, family = binomial(), cvControl = list(V = 3),
                             imputeAlgo = c("mean", "median", "mice"),
                             SL.library = c("SL.mean", "SL.glmnet", learners$names, "SL.ranger"))

# Review results.
summary(cv_sl)
```

We see here that `mtry = 2` performed a little bit better than `mtry = 1` or `mtry = 5`, although the difference is not significant. If we used more data and more cross-validation folds we might see more drastic differences. A higher `mtry` does better when a small percentage of variables are predictive of the outcome, because it gives each tree a better chance of finding a useful variable.

Note that `SL.ranger` and `SL.ranger_2` have the same settings, and their performance is very similar - statistically a tie. It's not exactly equivalent due to random variation in the two forests.

A key difference with missSuperLearner over caret or other frameworks is that we are not trying to choose the single best hyperparameter or model. Instead, we usually want the best weighted average. So we are including all of the different settings in our missSuperLearner, and we may choose a weighted average that includes the same model multiple times but with different settings. That can give us better performance than choosing only the single best settings for a given algorithm, which has some random noise in any case.

# Multicore parallelization

The `missSuperLearner` inherits the multicore parallelization feature from the `SuperLearner`. It is easy to use multiple CPU cores on your computer to speed up the calculations. We first need to setup `R` for multiple cores, then tell `CV.missSuperLearner` to divide its computations across those cores.

Here we show a example using the "snow" system.
```{r, eval=do_it_compile}
cl <- parallel::makeCluster(2, type = "PSOCK") # can use different types here
parallel::clusterSetRNGStream(cl, iseed = 2343)
testSNOW <- CV.missSuperLearner(Y = y_train, X = x_train, family = binomial(), cvControl = list(V = 3),
                                imputeAlgo = c("mean", "median", "mice"),
                                SL.library = c("SL.mean", "SL.glmnet", "SL.ranger"),
                                parallel = cl)
summary(testSNOW)
stopCluster(cl)
```


# Weight distribution for SuperLearner

The weights or coefficients of the missSuperLearner are stochastic - they will change as the data changes. So we don't necessarily trust a given set of weights as being the "true" weights, but when we use `CV.missSuperLearner` we at least have multiple samples from the distribution of the weights.

We can write a little function to extract the weights at each `CV.missSuperLearner` iteration and summarize the distribution of those weights. This may be added to the `missSuperLearner` package sometime in the future.

```{r, eval=do_it_compile}
# Review meta-weights (coefficients) from a CV.missSuperLearner object
review_weights = function(cv_sl) {
  meta_weights = coef(cv_sl)
  means = colMeans(meta_weights)
  sds = apply(meta_weights, MARGIN = 2,  FUN = sd)
  mins = apply(meta_weights, MARGIN = 2, FUN = min)
  maxs = apply(meta_weights, MARGIN = 2, FUN = max)
  # Combine the stats into a single matrix.
  sl_stats = cbind("mean(weight)" = means, "sd" = sds, "min" = mins, "max" = maxs)
  # Sort by decreasing mean weight.
  sl_stats[order(sl_stats[, 1], decreasing = TRUE), ]
}

print(review_weights(cv_sl), digits = 3)
```

Notice that in this case the ensemble never uses the `mean` nor the `ranger` with `mtry = 1`. Also the LASSO (`glmnet`) was only used on a subset of the folds. Adding multiple configurations of ranger was helpful because `mtry = 2` was used. However, based on the minimum column we can see that no algorithm was used every single time.

We recommend reviewing the weight distribution for any `missSuperLearner` project to better understand which algorithms are chosen for the ensemble.

# Feature selection (screening)

When datasets have many covariates our algorithms may benefit from first choosing a subset of available covariates, a step called feature selection. Then we pass only those variables to the modeling algorithm, and it may be less likely to overfit to variables that are not related to the outcome.

Let's revisit `SuperLearner::listWrappers()` and check out the bottom section.
```{r, eval=do_it_compile}
SuperLearner::listWrappers()
```

```{r, eval=do_it_compile}
# Review code for corP, which is based on univariate correlation.
SuperLearner::screen.corP
```

```{r, eval=do_it_compile}
set.seed(1)

# Fit the missSuperLearner.
# We need to use list() instead of c().
cv_sl <- CV.missSuperLearner(Y = y_train, X = x_train, family = binomial(),
                             # For a real analysis we would rather use V = 10.
                             cvControl = list(V = 3),
                             parallel = "multicore",
                             imputeAlgo = c("mean", "median", "mice"),
                             SL.library = list("SL.mean", "SL.glmnet", c("SL.glmnet", "screen.corP")))
summary(cv_sl)
```

We see a small performance boost by first screening by univarate correlation with our outcome, and only keeping variables with a p-value less than 0.10. Try using some of the other screening algorithms as they may do even better for a particular dataset.

# Optimize for AUC

For binary prediction we are typically trying to maximize AUC, which can be the best performance metric when our outcome variable has some imbalance. In other words, we don't have exactly 50% 1s and 50% 0s in our outcome. Our missSuperLearner is not targeting AUC by default, but it can if we tell it to by specifying our method.

```{r, eval=do_it_compile}
set.seed(1)

cv_sl <- CV.missSuperLearner(Y = y_train, X = x_train, family = binomial(),
                             # For a real analysis we would rather use V = 10.
                             cvControl = list(V = 3),
                             method = "method.AUC",
                             imputeAlgo = c("mean", "median", "mice"),
                             SL.library = list("SL.mean", "SL.glmnet", c("SL.glmnet", "screen.corP")))
summary(cv_sl)
```

This conveniently shows us the AUC for each algorithm without us having to calculate it manually. But we aren't getting SEs sadly.

Another important optimizer to consider is negative log likelihood, which is intended for binary outcomes and will often work better than NNLS (the default). This is specified by method = "NNloglik".

# References

Kennedy, C. (2017). **Guide to SuperLearner.** *R-Project*. https://cran.r-project.org/web/packages/SuperLearner/vignettes/Guide-to-SuperLearner.html


