---
title: "Introduction to missSuperLearner"
author: Pan Zhao, Nicolas Gatulle, Julie Josse and Antoine Chambaz
date: September 5, 2022
output: 
  rmarkdown::html_vignette:
    toc: true
    toc_depth: 2
    number_sections: true
vignette: >
  %\VignetteIndexEntry{Introduction to missSuperLearner}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Introduction

This vignette introduces the basic concepts for the super learning algorithm that 
is able to handle missing data, which is implemented in the `missSuperLearner` package.

# Basic Setup

In this section, we introduce the notation and review the super learning algorithm [1, 2].

## Notation

Observe the independent and identically distributed data points $O_i = (Y_i, X_i), i = 1, ... ,n$, 
where $Y$ is the outcome of interest and $X$ is a $p$-dimensional covariate. 
The objective is to estimate the function $f_0(X) = E(Y \,|\, X)$. The function can be expressed 
as the minimizer of the expected loss:
$$f_0(X) = \arg\min_f E[L(O, f(X))]$$
where the loss function is, for example, the squared error loss $L_2 = (Y - f(X))^2$.

Let $P_n$ denote the empirical distribution over the data. 

## Super Learning

For a given problem, a library of prediction algorithms can be proposed. A library is simply a collection of algorithms.
The algorithms in the library should come from contextual knowledge and a large set of default
algorithms. We use algorithm in the general sense as any mapping from the data into a predictor.
The algorithms may range from a simple linear regression model to a multi-step algorithm involving
screening covariates, optimizing tuning parameters, and selecting a working model among a large
class of candidate working models. As long as the algorithm takes the observed data and outputs
a predicted value we consider it a prediction algorithm. For example, the library may include least
squares regression estimators for a large class of regression working models indexed by subsets of
the covariates, algorithms indexed by set values of the fine-tuning parameters for a collection of
values, algorithms using internal cross-validation to set fine-tuning parameters, algorithms coupled
with screening procedures to reduce the dimension of the covariate vector, and so on. 

Denote the library $\mathcal{L}$ and the cardinality of $\mathcal{L}$ as $K$. Consider the super learning algorithm:

1. Fit each algorithm in the library $\mathcal{L}$ on the entire data set $\mathcal{O} = \{O_i : i = 1, \ldots, n\}$ to 
estimate $\hat{f}_k (X), k = 1, \ldots, K$.

2. Split the data set $\mathcal{O}$ into a training and validation sample, according 
to a $V$-fold cross-validation scheme: split the ordered $n$ observations into $V$ equal-size 
groups, let the $v$-th group be the validation sample, and the remaining group the training 
sample, $v = 1, \ldots, V$. Define $T(v)$ to be the $v$-th training data split and $V(v)$ 
to be the corresponding validation data split. $T(v) = \mathcal{O} \backslash V(v), v = 1, \ldots, V$

3.  For the $v$-th fold, fit each algorithm in $\mathcal{L}$ on $T(v)$ and save the 
predictions on the corresponding validation data, $\hat{f}_{k,T(v)} (X_i), O_i \in V(v), v = 1, \ldots, V$.

4. Stack the predictions from each algorithm together to create a $n \times K$ matrix 
$Z =\left\{\hat{f}_{k,T(v)} (X_{V(v)}), v = 1, \ldots, V, k = 1, \ldots, K\right\}$, where we 
used the notation $X_{V(v)} = (X_i: O_i \in V(v))$ for the covariate vectors of the $V(v)$-validation sample.

5. Propose a family of weighted combinations of the candidate estimators indexed 
by weight vector $\alpha$: 
$$m(z|a) = \sum_{k=1}^K \alpha_k \, \hat{f}_{k,T(v)} (X_{V(v)}), \alpha_k \geq 0 \, \forall k, \sum_{k=1}^K \alpha_k = 1.$$
6. Determine the $\alpha$ that minimizes the cross-validated risk of the candidate 
estimator $\sum_{k=1}^K \alpha_k \, \hat{f_k}$ over all allowed $\alpha$-combinations:
$$\hat{\alpha} = \arg\min_{\alpha} \sum_{i=1}^n (Y_i - m(z_i | \alpha))^2.$$
7. Combine $\hat{\alpha}$ with $\hat{f_k} (X), k = 1, \ldots, K$ according to the family $m(z|a)$ of weighted combinations to create the final super learner fit: 
$$\hat{f}_{SL} (X) = \sum_{k=1}^K \hat{\alpha}_k \, \hat{f_k} (X).$$

In summary, Super learner performs asymptotically as well as best possible weighted combination.

# Super Learning with Missing Data

Missing data are ubiquitous in many real data analysis tasks. There exists extensive 
literature on **imputation** and inference with missing data [3]. The *Missing At Random* (MAR) assumption
is mostly used in theory and practices, which allows us to maximize the likelihood of observed data 
without considering the missing mechanism. When the data is *Missing Not At Random*, 
where missingness depends on the unobserved values, the identification problem complicates the analysis.

Learning predictive models with missing values poses distinct challenges compared to inference
tasks [4]. The most common practice however remains by far to use off-the-shelf methods first for imputation of
missing values and second for supervised-learning on the resulting completed data.

A systematic analysis of Impute-the-Regress procedures in a general setting: non-linear
response function and any missingness mechanism (no MAR assumptions) in [5] shows that
Impute-then-Regress procedures are Bayes optimal for all missing data mechanisms and for
almost all imputation functions, whatever the number of variables that may be missing.

## Imputation

There are many popular imputation methods, such as `softImpute` (matrix completion via iterative soft-thresholded SVD) 
and `missForest` (nonparametric missing value imputation using random forest). 
See the [R-miss-tastic](https://rmisstastic.netlify.app/) website for comprehensive references.
However, in order to incorporate the imputation step into the cross-validation scheme,
three methods are implemented in our package.

* **Mean Imputation** For $v = 1, \ldots, V$, we use the mean of the training sample $T(v)$ to 
impute the missing values. Note that for categorical/integer variables, the mode is used instead.

* **Median Imputation** For $v = 1, \ldots, V$, we use the median of the training sample $T(v)$ to 
impute the missing values. Note that for categorical/integer variables, the mode is used instead.

* **Multivariate Imputation by Chained Equations (MICE)** The `mice` package implements a method 
to deal with missing data, and creates multiple imputations for multivariate missing data. The method 
is based on *Fully Conditional Specification*, where each incomplete variable is imputed by a separate 
model. Note that here we only use the training sample to fit the models.


## Super Learning Algorithm that Handles Missing Data

We consider the same super learning algorithm as introduced in Section 2.2, except that 
the functions class is
$$\mathcal{F}^\ast = \left\{f^\ast : f^\ast (X^\ast) = f^\ast_{c} \circ f^\ast_{b} \circ f^\ast_{a} (X^\ast) \right\},$$
where $X^\ast$ denotes the covariates with missing values, and $f^\ast_{a}, f^\ast_{b}, f^\ast_{c}$ are imputation,
screening and prediction functions, respectively.

# References

[1] Van der Laan, M. J., Polley, E. C., & Hubbard, A. E. (2007). **Super learner.** *Statistical applications in genetics and molecular biology*, 6(1). https://doi.org/10.2202/1544-6115.1309

[2] Polley, E. C., & Van Der Laan, M. J. (2010). **Super learner in prediction.** *U.C. Berkeley Division of Biostatistics Working Paper Series*. Paper 226. https://biostats.bepress.com/ucbbiostat/paper266/

[3] Little, R. J., & Rubin, D. B. (2019). **Statistical analysis with missing data** (Vol. 793). John Wiley & Sons.

[4] Josse, J., Prost, N., Scornet, E., & Varoquaux, G. (2019). **On the consistency of supervised learning with missing values.** *arXiv preprint* [arXiv:1902.06931.](https://arxiv.org/abs/1902.06931)

[5] Le Morvan, M., Josse, J., Scornet, E., & Varoquaux, G. (2021). **Whatâ€™s a good imputation to predict with missing values?.** *Advances in Neural Information Processing Systems*, 34, 11530-11540. https://arxiv.org/abs/2106.00311


